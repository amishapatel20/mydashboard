import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")

df2 = pd.read_csv("/content/report.csv", encoding='latin1')
df2
import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn


import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
from plotly import tools
import gc
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score

# Assuming df2 is your DataFrame loaded from the CSV

# Calculate total crimes and add it as a new column to the DataFrame
df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

ax = plt.figure(figsize=(10, 22))

# Now use the DataFrame and specify x and y columns
ax = sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y")

ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
df2["relative_crime"]=(df2["violent_crimes"]+df2["homicides"]+
                 df2["rapes"]+df2["assaults"]+df2["robberies"])/df2.population
ax=plt.figure(figsize=(10,22))
ax = sns.barplot(x=df2["relative_crime"],y=df2["agency_jurisdiction"],color="b")
ax.set_title('Total number of crimes in US during in 40 years')
ax.set(xlabel='Total number of crimes', ylabel='City and state')
i = df2[(df2.agency_jurisdiction == "United States")].index
df2.drop(i,inplace=True)
import plotly.graph_objs as go
from plotly.offline import plot

# List of unique jurisdictions
# Changed 'df' to 'df2'
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    # Changed 'df' to 'df2'
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and plot
fig = dict(data=data, layout=layout)
plot(fig)  # Use plot() for better compatibility with Google Colab
!pip install plotly --upgrade

import pandas as pd
df2 = pd.read_csv('/content/report.csv')  # Adjust path if needed

import pandas as pd
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode

# Initialize Plotly for offline use within the notebook
init_notebook_mode(connected=True)

# Load the dataset and assign it to df2 if not already done
df2 = pd.read_csv('/content/report.csv')  # Adjust path as required

# List of unique jurisdictions
jurisdictions = df2['agency_jurisdiction'].unique()

# Create traces dynamically for each jurisdiction
data = []
for jurisdiction in jurisdictions:
    subset = df2[df2['agency_jurisdiction'] == jurisdiction]
    trace = go.Scatter(
        x=subset['report_year'],
        y=subset['violent_crimes'],
        name=jurisdiction,
        opacity=0.8
    )
    data.append(trace)

# Define layout
layout = dict(
    title="Total Crimes in US over 40 years",
    xaxis=dict(title='Year'),
    yaxis=dict(title='Violent Crimes Reported'),
)

# Create figure and display plot inline using fig.show()
fig = go.Figure(data=data, layout=layout)
fig.show()  # Using show() for inline plotting in Colab

!pip install plotly cufflinks --upgrade

import pandas as pd
import numpy as np
import plotly.graph_objs as go
import cufflinks as cf
from plotly.offline import init_notebook_mode

# Enable offline mode and initialize notebook mode for cufflinks
init_notebook_mode(connected=True)
cf.go_offline()

!pip install plotly --upgrade

import pandas as pd
import plotly.express as px

# Load your data
df2 = pd.read_csv('/content/report.csv')

# Use plotly express for a quick line plot with multiple lines (jurisdictions)
fig = px.line(
    df2,
    x='report_year',
    y='violent_crimes',
    color='agency_jurisdiction',
    title="Total Crimes in US over 40 years"
)
fig.show()

!pip install psutil  # orca dependency

# Check for NaN values
print(df2[['report_year', 'violent_crimes']].isna().sum())

# Ensure 'report_year' is numeric
df2['report_year'] = pd.to_numeric(df2['report_year'], errors='coerce')
df2['violent_crimes'] = pd.to_numeric(df2['violent_crimes'], errors='coerce')

a = set(df2["agency_jurisdiction"])
a = list(a)

doubles = {}
for i in range(len(a)):
    doubles[i] = df2[df2['agency_jurisdiction'].str.contains(a[i])]

traces = []
for i in range(len(a)):
    trace = go.Scatter(
        x=doubles[i]['report_year'],
        y=doubles[i]['violent_crimes'],
        name=a[i],
        opacity=0.8
    )
    traces.append(trace)

layout = go.Layout(
    title="Total Crimes in US during in 40 years",
    xaxis=dict(title='Time Span'),
    yaxis=dict(title='Cumulative crimes')
)

fig = go.Figure(data=traces, layout=layout)
fig.show()

!pip install plotly

NYC = df2[df2.agency_jurisdiction == 'New York City, NY']

ax=plt.figure(5,figsize=(18,12))
ax = plt.plot([1975, 1975], [0, 193000], 'y-', lw=2)
ax = plt.text(1975, 193000, 'Abraham Beame',color='blue',horizontalalignment='center')
ax = plt.plot([1978, 1978], [0, 193000], 'r-', lw=2)
ax = plt.text(1978, 193000, 'Ed Koch',color='blue',horizontalalignment='left')
ax = plt.plot([1990, 1990], [0, 193000], 'g-', lw=2)
ax = plt.text(1990, 193000, 'David Dinkins',color='blue',horizontalalignment='center')
ax = plt.plot([1994, 1994], [0, 193000], 'y-', lw=2)
ax = plt.text(1994, 193000, 'Rudy Giuliani',color='red',horizontalalignment='center')
ax = plt.plot([2002, 2002], [0, 193000], 'r-', lw=2)
ax = plt.text(2002, 193000, 'Michael Bloomberg',color='red',horizontalalignment='center')
ax = plt.plot([2014, 2014], [0, 193000], 'g-', lw=2)
ax = plt.text(2014, 193000, 'Bill de Blasio',color='blue',horizontalalignment='center')
ax = plt.plot(NYC["report_year"],NYC["violent_crimes"])
plt.title('Crimes while mayors')
plt.xlabel("Year")
plt.ylabel("Sum of all crimes")
plt.ylim([0,220000])
plt.xlim([1970,2020])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
ax = sns.barplot(x="report_year", y="homicides", data=NYC, palette="cubehelix")
plt.ylabel("Number of Homicides")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="rapes", data=NYC, palette="cubehelix")
plt.ylabel("Number of Rapes")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="assaults", data=NYC, palette="cubehelix")
plt.ylabel("Number of Assaults")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15, 10))
plt.xticks(rotation='vertical')
sns.barplot(x="report_year", y="robberies", data=NYC, palette="cubehelix")
plt.ylabel("Number of Robberies")
plt.show()

df3 = pd.read_csv("/content/output.csv.zip", encoding='latin1')
df3.head()
df3['County']=df3['County'].astype(str).str.split().str[0]
df3['County']
df3.rename(columns={'Rate':'Unemployment rate'}, inplace=True)
df3.head()
df3.isna().sum().sum()
df4 = pd.read_csv("/content/price.csv.zip", encoding='latin1')
df4.head()
df4.describe()
years = list(set([y.split( )[1] for y in df4.columns[6:]]))
months = df4.columns[6:]
nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

# Convert all month columns to numeric, replacing non-numeric entries with NaN
for month in months:
    nyc_rent[month] = pd.to_numeric(nyc_rent[month], errors='coerce')

# Now try the original np.nanmedian plot
trace1 = go.Scatter(
    x=months,
    y=np.nanmedian(nyc_rent[nyc_rent["State"] == "NY"][months], axis=0),
    mode='markers',
    marker=dict(size=5, color='aqua'),
    name="New York"
)

fig = go.Figure(data=[trace1])
fig.update_layout(
    title="Median NY Rent After Cleaning",
    xaxis_title="Months",
    yaxis_title="Rent"
)
fig.show()

!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Reset index to ensure alignment if months are used as index
ny_rent = ny_rent.reset_index(drop=True)

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
months = months[:len(median_rent)]  # Adjust 'months' if necessary

fig = px.line(
    x=months,  # Use months as x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
!pip install plotly --upgrade

import plotly.express as px

# Filter for NY data
ny_rent = nyc_rent[nyc_rent["State"] == "NY"]

# Calculate median rent for each month across all NY entries
median_rent = ny_rent[months].median(axis=0)

# Ensure 'months' and 'median_rent' have the same length
# The error is likely caused by a mismatch in lengths
# between 'months' and the calculated median rents.
# We need to make sure both have the same length.
months = months[:len(median_rent)]  # Adjust 'months' to match median_rent length

fig = px.line(
    x=months,  # Use the adjusted 'months' for the x-axis
    y=median_rent,  # Use calculated median rent as y-axis
    title="Median NY Rent",
    markers=True
)
fig.show()
# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Save figure as HTML and then open it directly
fig.write_html("median_rent_plot.html")

# Display link to download and open the plot
from IPython.display import FileLink
FileLink("median_rent_plot.html")

# Check `months`
print("Months:", months)

# Check the head of the DataFrame and the columns in `nyc_rent`
print("DataFrame Head:\n", nyc_rent.head())
print("DataFrame Columns:\n", nyc_rent.columns)

# Check the unique values in the 'State' column
print("Unique States:", nyc_rent["State"].unique())

nyc_rent = df4[df4["Metro"]=="New York"]
ny_njer_rent = df4.groupby("State")[months].median()
df4.isna().sum().sum()
df4[df4.isnull().any(axis=1)].head()
import seaborn as sns

# Now you can use sns to create the heatmap
sns.heatmap(df4.isnull(), cbar=False)
df4.interpolate(method='piecewise_polynomial', inplace=True, limit_direction="both")
df4
df4.isna().sum().sum()
df4.interpolate(method='linear', inplace=True, limit_direction="both")
df4.drop(["Metro"], axis=1,inplace=True)
df4.isna().sum().sum()
df2.isna().sum().sum()
sns.heatmap(df2.isnull(), cbar=False)
df2=df2[df2["report_year"]>2009]
df2.drop(["months_reported","agency_code"], axis=1,inplace=True)
df2[df2.isnull().any(axis=1)]
df2.dropna(inplace=True)
df2.isna().sum().sum()
df2.head()
df2[['City',"State"]] = df2["agency_jurisdiction"].str.split(',',expand=True)
df2.drop(["City","agency_jurisdiction"], axis=1,inplace=True)
df2.head()
d = {'State': ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois", "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania", "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"], 'Abb': ["AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"]}
df_abv = pd.DataFrame(data=d)
df_abv.head()
df3=df3.merge(df_abv)
df3.drop(["State"], axis=1,inplace=True)
df3.rename(columns={'Abb':'State'}, inplace=True)
df3.head()
df = df3.merge(df4)
df.drop(["Month","County","City Code","Population Rank","City"], axis=1,inplace=True)
df=df[df["Year"]>2009]
df.State = df.State.astype(str)
df2.State = df2.State.astype(str)
df.State=df.State.str.strip()
df2.State=df2.State.str.strip()
df=df.groupby(["State","Year"]).mean()
df.reset_index(inplace=True)
df.head()
final_df=df2.merge(df)
del df2,df3,df4,df
gc.collect()
gc.collect()
gc.collect()
gc.collect()
X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
Y_train_set_df=final_df[final_df["Year"]<2015].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
Y_test_set_df=final_df[final_df["Year"]>2014].drop(['report_year', 'population', 'violent_crimes', 'homicides', 'rapes',
       'assaults', 'robberies', 'crimes_percapita', 'homicides_percapita',
       'rapes_percapita', 'assaults_percapita', 'robberies_percapita',
       'State', 'Year', 'Unemployment rate', 'November 2010',
       'December 2010', 'January 2011', 'February 2011', 'March 2011',
       'April 2011', 'May 2011', 'June 2011', 'July 2011', 'August 2011',
       'September 2011', 'October 2011', 'November 2011', 'December 2011',
       'January 2012', 'February 2012', 'March 2012', 'April 2012', 'May 2012',
       'June 2012', 'July 2012', 'August 2012', 'September 2012',
       'October 2012', 'November 2012', 'December 2012', 'January 2013',
       'February 2013', 'March 2013', 'April 2013', 'May 2013', 'June 2013',
       'July 2013', 'August 2013', 'September 2013', 'October 2013',
       'November 2013', 'December 2013', 'January 2014', 'February 2014',
       'March 2014', 'April 2014', 'May 2014', 'June 2014', 'July 2014',
       'August 2014', 'September 2014', 'October 2014', 'November 2014',
       'December 2014', 'January 2015', 'February 2015', 'March 2015',
       'April 2015', 'May 2015', 'June 2015', 'July 2015', 'August 2015',
       'September 2015', 'October 2015', 'November 2015', 'December 2015',
       'January 2016', 'February 2016', 'March 2016', 'April 2016', 'May 2016',
       'June 2016', 'July 2016', 'August 2016', 'September 2016',
       'October 2016', 'November 2016', 'December 2016', 'January 2017'], axis=1)
!pip install scikit-learn

import sklearn
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

def train_test(model, X_train, X_test, y_train, y_test):
    """
    Trains a model and evaluates its performance on training and testing data.

    Args:
        model: The model to be trained and evaluated.
        X_train: The training data features.
        X_test: The testing data features.
        y_train: The training data target variable.
        y_test: The testing data target variable.

    Returns:
        None. Prints the R-squared and Mean Squared Error for both training and testing sets.
    """
    # Predictions on the train set
    y_train_pred = model.predict(X_train)

    # Predictions on the test set
    y_test_pred = model.predict(X_test)

    # Evaluation metrics
    print('R-squared for training set: %.2f' % r2_score(y_train, y_train_pred))
    print('Mean Squared Error for training set: %.2f' % mean_squared_error(y_train, y_train_pred))
    print('R-squared for test set: %.2f' % r2_score(y_test, y_test_pred))
    print('Mean Squared Error for test set: %.2f' % mean_squared_error(y_test, y_test_pred))


X_train_set_df=final_df[final_df["Year"]<2015].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_train_set_df
Y_train_set_df=final_df[final_df["Year"]<2015][['violent_crimes']] # Keep only the target column


# ipython-input-215-b371bbea1648
X_test_set_df=final_df[final_df["Year"]>2014].drop(["State","report_year","Year"], axis=1)
# Assuming 'violent_crimes' is your target variable, keep it in Y_test_set_df
Y_test_set_df=final_df[final_df["Year"]>2014][['violent_crimes']]
ENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(X_train_set_df, Y_train_set_df)
train_test(ENSTest, X_train_set_df, X_test_set_df, Y_train_set_df, Y_test_set_df)
df = pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0], encoding='latin1')
df.head()
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)
un_crimes=pivoted
pivoted.head()
pivoted=pivoted.fillna(0)
pivoted.loc[pivoted.index.get_level_values(0)]
print(pivoted.columns)

# Define the expected columns
desired_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Assuming 'pivoted' contains your data, create 'pivoted_max'
# Convert desired columns to numeric, handling errors by coercing to NaN
for col in desired_columns:
    pivoted[col] = pd.to_numeric(pivoted[col], errors='coerce') # Convert columns to numeric, forcing non-numeric values to NaN

# Now perform the groupby and aggregation
pivoted_max = pivoted.groupby('Region')[desired_columns].max() # Creating pivoted_max by grouping by 'Region' and finding the maximum values for the desired columns within each region.

# Add missing columns to `pivoted_max` with a default value of 0 or NaN
for col in desired_columns:
    if col not in pivoted_max.columns:
        pivoted_max[col] = 0  # or use `float('nan')` if you prefer missing values

# After this, you should have all 8 columns in `pivoted_max`
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
table = ax.table(
    cellText=pivoted_max[desired_columns].values,  # Use DataFrame values for the table cells
    colLabels=desired_columns,  # Set the column headers
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(desired_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()

print(pivoted.info())  # Check if the columns are present
print(pivoted[["Kidnapping at the national level, rate per 100,000",
               "Percentage of male and female intentional homicide victims, Female",
               "Percentage of male and female intentional homicide victims, Male",
               "Theft at the national level, rate per 100,000 population",
               "Total Sexual Violence at the national level, rate per 100,000"]].head())

import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.
import pandas as pd

# Adjust the groupby step to ensure all required columns are included and converted to numeric
pivoted_max = pivoted.groupby('Region')[[
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]].apply(lambda x: x.apply(pd.to_numeric, errors='coerce')).max().fillna(0)
# Apply pd.to_numeric to each column within the group using another apply
# errors='coerce' handles any values that cannot be converted by replacing them with NaN (Not a Number).
# Finally, fillna(0) replaces the NaN values with 0.


# Convert pivoted_max to a DataFrame with a single row and the desired columns
pivoted_max = pd.DataFrame([pivoted_max], columns=pivoted_max.index)
pivoted_max = pivoted_max.T.rename(columns={0: 'Value'}) #Transposing and Renaming column

# Define a list of columns that must be in the DataFrame
required_columns = [
    "Intentional homicide rates per 100,000",
    "Robbery at the national level, rate per 100,000 population",
    "Assault rate per 100,000 population",
    "Kidnapping at the national level, rate per 100,000",
    "Percentage of male and female intentional homicide victims, Female",
    "Percentage of male and female intentional homicide victims, Male",
    "Theft at the national level, rate per 100,000 population",
    "Total Sexual Violence at the national level, rate per 100,000"
]

# Add missing columns with 0 values if they don't exist in `pivoted_max`
#Since pivoted_max is now a dataframe, we can use .reindex
pivoted_max = pivoted_max.reindex(required_columns, fill_value=0)
import matplotlib.pyplot as plt

# Plot the DataFrame as a table
fig, ax = plt.subplots(figsize=(23, 8))  # Adjust the figure size to 23x8 inches

# Hide axes
ax.axis('off')  # Turns off the grid and axis

# Create the table
# pivoted_max is now indexed by required_columns, so we get values directly
# and reshape to a single-row array for the table
table = ax.table(
    cellText=pivoted_max.values.reshape(1, -1),  # Reshape values to a single-row array
    colLabels=pivoted_max.index,  # Set column headers to the index (required_columns)
    cellLoc='center',  # Center-align text in cells
    loc='center'  # Position the table in the center of the plot
)

# Format the table
table.auto_set_font_size(False)  # Turn off automatic font sizing
table.set_fontsize(10)  # Set a consistent font size for readability
table.auto_set_column_width(col=list(range(len(required_columns))))  # Adjust column widths

# Display title (optional)
plt.title("Crime Rate Data by Region (8 Columns)")

plt.show()
import pandas as pd

# Load the CSV file with encoding adjustment (if needed, e.g., for special characters)
file_path = '/content/Crimes_UN_data.csv'  # Replace with the actual path to your file
data = pd.read_csv(file_path, encoding='ISO-8859-1', skiprows=2)

# Rename columns for clarity (modify as necessary based on your CSV structure)
data.columns = ["Index", "Region", "Year", "Series", "Value", "Footnotes", "Source"]

# Pivot the data so that each type of crime rate becomes a separate column
pivoted = data.pivot_table(index='Region', columns='Series', values='Value', aggfunc='first')

# Convert columns to numeric, coercing any non-numeric values to NaN
pivoted = pivoted.apply(pd.to_numeric, errors='coerce')

# Fill NaN values with 0 for consistency
pivoted = pivoted.fillna(0)

# Ensure specific columns are in float format, handling missing columns gracefully
for col in ["Intentional homicide rates per 100,000",
            "Robbery at the national level, rate per 100,000 population",
            "Assault rate per 100,000 population"]:
    if col in pivoted.columns:
        pivoted[col] = pivoted[col].astype(float)

# Sort by "Intentional homicide rates per 100,000" in descending order
pivoted_sorted = pivoted.sort_values(by="Intentional homicide rates per 100,000", ascending=False)

# Display the first few rows to verify the output
print(pivoted_sorted.head())

# Check if the pivoted data has the shape 230x8; if not, slice accordingly
if pivoted_sorted.shape[0] >= 230 and pivoted_sorted.shape[1] >= 8:
    pivoted_230x8 = pivoted_sorted.iloc[:230, :8]
else:
    pivoted_230x8 = pivoted_sorted

print(pivoted_230x8)

import pandas as pd
import matplotlib.pyplot as plt

# ... (your previous code to load and process data) ...

# Assuming 'pivoted_230x8' contains the data you want to work with
pivoted_max = pivoted_230x8  # Assign pivoted_230x8 to pivoted_max

# Sort by 'Intentional homicide rates per 100,000' and get the top 15
pivoted_max1 = pivoted_max.sort_values(
    by="Intentional homicide rates per 100,000", ascending=False
).head(15)

# Plot 'Intentional homicide rates per 100,000'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max1['Intentional homicide rates per 100,000'].plot(
    kind='bar', rot=90, title='Intentional homicide rates per 100,000'
)

# Sort by 'Robbery at the national level, rate per 100,000 population' and get the top 15
pivoted_max2 = pivoted_max.sort_values(
    by="Robbery at the national level, rate per 100,000 population", ascending=False
).head(15)

# Plot 'Robbery at the national level, rate per 100,000 population'
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max2['Robbery at the national level, rate per 100,000 population'].plot(
    kind='bar', rot=90, title='Robbery at the national level, rate per 100,000 population'
)

plt.show()  # Add this to display the plots
pivoted_max3=pivoted_max.sort_values(by="Assault rate per 100,000 population",ascending=False)
fig, ax = plt.subplots(figsize=(20, 10))
pivoted_max3["Assault rate per 100,000 population"].head(15).plot(kind='bar', rot=90,  title='Assault rate per 100,000 population')
del pivoted_max3,pivoted_max,df,pivoted_max2,pivoted


gc.collect()

pd.options.display.max_rows = 20
pd.options.display.max_columns = 20
nm = pd.read_csv('/content/Nationmaster.csv.zip', encoding='latin1')
nm.rename(columns={'Unnamed: 0': "Country"}, inplace=1)
nm.shape
#dropping data that is irrelevant and
nm.drop(columns=[
	"Age of criminal responsibility %28notes%29", "Background",
	"Illicit drugs",
	"Minimum to serve before eligibility for requesting parole",
	"Prosecution rate",
	"Prosecution rate per million",
	"%25 sales"
], inplace=True)

print("deleting average countries: \n", nm[nm["Country"].str.match(".*average")]["Country"])
nm.drop(nm[nm["Country"].str.match(".*average")].index, inplace=True)
#dropping countries we do not have data about
limit = 20  # drop all countries with data in less than x columns
bad_countries = (~nm.isna()).sum(axis=1)
print("deleting ", (bad_countries < 20).sum())
nm.drop(nm[bad_countries < 20].index, inplace=True)
nm.shape
nm[nm["Country"].str.match(".*Un.*")]  # country name search
nm2 = nm.copy()
nm = nm2.copy()
limit = 60  # drop columns with more than x countries missing
bad_cols = nm.isna().sum(axis=0)
print("deleting ", (bad_cols > limit).sum(), ":\n", nm.columns[bad_cols > limit])
nm.drop(columns=nm.columns[bad_cols > limit],inplace=True)
print(nm.shape)
nm

import plotly.offline as py

import plotly.graph_objs as go

py.init_notebook_mode(connected=True)
countriesraw = pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries=countriesraw.copy()

countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY"])

nm.reset_index(inplace=True)

nm=nm.merge(countries,how="outer")

nm.sort_index(inplace=True)
nm.set_index("Country",inplace=True)
nm.columns
column_name='Total crimes per 1000'
data = [ dict(
        type = 'choropleth',
        locations = nm['CODE'],
        z = nm[column_name].fillna(0),
        text = nm.index,
        colorscale = [[0,"rgb(5, 10, 172)"],[0.35,"rgb(40, 60, 190)"],[0.5,"rgb(70, 100, 245)"],\
            [0.6,"rgb(90, 120, 245)"],[0.7,"rgb(106, 137, 247)"],[1,"rgb(220, 220, 220)"]],
        autocolorscale = False,
        reversescale = True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = False,
            title = column_name),
      ) ]

layout = dict(
    title = 'world plot of '+column_name,
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )

nm[[column_name]].sort_values(by=column_name,ascending=0)
import pandas as pd

unodc = None
fields = [
    "Assault", "Kidnapping", "Theft", "Robbery", "Burglary",
    "Domestic_Burglary", "Theft_of_Private_Cars", "Motor_Vehicle_Theft",
    "Total_Sexual_Violence", "Rape", "Sexual_Offences_ag_Children"
]
string = "abcdefghijk"

for i in range(len(string)):
    l = string[i]
    # Construct the `names` list correctly
    names = ["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]

    # Update the file path string correctly (without using `%` for formatting)
    tmp = pd.read_csv(
        '/content/Publication Reports_1a.csv',  # Adjusted to the correct file path
        sep=";", index_col=[2], header=None, skipinitialspace=True,
        skiprows=10, encoding='latin1',
        names=[
            "Region", "Sub-Region", "Country",
            *["%s_Abs_20%02d" % (fields[i], j) for j in range(3, 15)],
            "nothing",
            *["%s_Rel_20%02d" % (fields[i], j) for j in range(3, 15)]
        ]
    )

    # Reset index and process the `Country` column
    tmp.reset_index(inplace=True)
    tmp["Country"] = tmp["Country"].str.replace("*", "", regex=False)\
        .str.replace(" of America", "", regex=False)\
        .str.replace(" (England and Wales)", "", regex=False)\
        .str.replace("Russian Federation", "Russia", regex=False)\
        .str.replace("Republic of ", "", regex=False)\
        .str.replace(" (Plurinational State of)", "", regex=False)\
        .str.replace("Korea", "Korea, South", regex=False)

    # Set index and drop unnecessary columns
    tmp.set_index("Country", inplace=True)
    tmp.drop(columns=["Region", "Sub-Region", "nothing"], inplace=True)

    # Convert data columns to floats, handling European-style number formatting
    for n in names:
        tmp[n] = tmp[n].str.replace(".", "", regex=False)\
            .str.replace(",([0-9])$", lambda m: "." + m[1], regex=True).astype(float)

    # Drop rows with NaN index values and merge
    tmp = tmp.loc[tmp.index.dropna()]
    unodc = tmp if unodc is None else unodc.merge(tmp, how="outer", on="Country")

# Drop the "Country/territory" column if it exists
if "Country/territory" in unodc.columns:
    unodc.drop("Country/territory", inplace=True)

# You can uncomment the line below to rename a column if needed
# unodc.rename(columns={'Country/territory': 'Subcontinent'}, inplace=True)

countries=pd.read_csv('/content/2014_world_gdp_with_codes.csv')
countries["Country"]=countries["COUNTRY"]
countries=countries.drop(columns=["COUNTRY","GDP (BILLIONS)"])


#unodc["CC"]=countries.loc[unodc["Country"].str.replace("*","")].reset_index()["CODE"]
unodc=unodc.reset_index().merge(countries,how="outer")

unodc.sort_index(inplace=True)
unodc.set_index("Country",inplace=True)
import pandas as pd

unodc_mult = pd.DataFrame()
tmp_list = []  # List to store DataFrames for concatenation

for i in range(3, 15):
    # Extract and process the relevant columns
    tmp = unodc[["%s_Abs_20%02d" % (f, i) for f in fields] + ["%s_Rel_20%02d" % (f, i) for f in fields]]
    tmp.columns = tmp.columns.str.replace("_20%02d" % i, "", regex=False)
    tmp["Year"] = 2000 + i
    tmp = tmp.reset_index().set_index(["Country", "Year"])

    # Collect the processed DataFrame in a list
    tmp_list.append(tmp)

# Concatenate all DataFrames in the list
unodc_mult = pd.concat(tmp_list)

# Further processing of unodc_mult
unodc_mult = unodc_mult.sort_index()
unodc_mult = unodc_mult.reset_index()
unodc_mult.rename(columns={'Country': 'Region'}, inplace=True)
unodc_mult = unodc_mult.set_index(['Region', 'Year'])

# Display the first 15 rows
unodc_mult[:15]

df=pd.read_csv('/content/Crimes_UN_data.csv', skiprows=[0],thousands=",", encoding='latin1')
cols = list(df.columns)
cols[1] = 'Region'
df.columns = cols
pivoted = pd.pivot_table(df, values='Value', index=['Region', 'Year'], columns='Series', aggfunc=sum)

un_crimes=pivoted
un_crimes=un_crimes.rename({"United States of America": "United States"}, axis='index')
unodc_un=unodc_mult.merge(un_crimes, left_index=True, right_index=True,how="outer")
unodc_un.columns=unodc_un.columns.str.replace(" per 100,000"," Rel",regex=False).str.replace(" population","",regex=False)
unodc_un.index.levels[0][unodc_un.index.levels[0].str.match("United")]
import pandas as pd
import plotly.offline as py  # Assuming you are using plotly.offline for plotting

# ... (previous code) ...

column = fields[2]
year = 3
column_name = "%s_Rel_20%02d" % (column, year)
plot = unodc[column_name].fillna(0)  # .str.replace(',', '')

# Replace with the actual column name containing the country codes.
# Most likely it's part of the index or dropped earlier
# Assuming the column was named 'CountryCode' in the original dataset
CCs = unodc.index  # Use the index if 'Country' is the index
# or
# CCs = unodc['CountryCode']  # If there's a 'CountryCode' column

data = [dict(
    type='choropleth',
    locations=CCs,
    z=plot,
    text=unodc.index,
    colorscale=[[0, "rgb(5, 10, 172)"], [0.35, "rgb(40, 60, 190)"],
                [0.5, "rgb(70, 100, 245)"],
                [0.6, "rgb(90, 120, 245)"], [0.7, "rgb(106, 137, 247)"],
                [1, "rgb(220, 220, 220)"]],
    autocolorscale=False,
    reversescale=True,
    marker=dict(
        line=dict(
            color='rgb(180,180,180)',
            width=0.5
        )),
    colorbar=dict(
        autotick=False,
        title=column_name.replace("_", " ")),
)]

layout = dict(
    title='world plot of ' + column_name.replace("_", " "),
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection=dict(
            type='Mercator'
        )
    )
)

fig = dict(data=data, layout=layout)
py.iplot(fig, validate=False, filename='d3-world-map')
# plot.sort_values(ascending=False)[:20]
unodc_avg_year=pd.DataFrame()
unodc_avg=pd.DataFrame()
unodc_max_cat=pd.DataFrame()
for f in fields: # trying to use only these to not let rare crimes play big role, but no difference:
                #["Robbery","Assault","Kidnapping","Burglary","Theft_of_Private_Cars"]:
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc[names].mean(axis=1)
un_fields=['Assault rate Rel',
       'Intentional homicide rates Rel',
       'Kidnapping at the national level, rate Rel',
       'Percentage of male and female intentional homicide victims, Female',
       'Percentage of male and female intentional homicide victims, Male',
       'Robbery at the national level, rate Rel',
       'Theft at the national level, rate Rel',
       'Total Sexual Violence at the national level, rate Rel']
for f in un_fields:
    print(f)
    names=["%s_Rel_20%02d"%(f,j) for j in range(3,15)]
    unodc_avg_year[f]=unodc_un[f].unstack(level=1).astype(float).mean(axis=1)

unodc_avg_year-=unodc_avg_year.mean()
unodc_avg_year/=unodc_avg_year.std()
unodc_avg["crime_score"]=unodc_avg_year.mean(axis=1)
unodc_avg["crime_score"]-=unodc_avg["crime_score"].min()
unodc_avg["crime_score"].fillna(0,inplace=True)
# Assuming 'CODE' is in the index of 'unodc' and you want to add it as a column in 'unodc_avg'
unodc_avg["CODE"] = unodc.index # Assuming 'CODE' is the index
# Alternatively, if 'CODE' exists as a column in your original 'unodc' DataFrame but is dropped
# during processing, you'll need to modify your code to keep that column or find a way to reintroduce it.
# Example: If 'CODE' was dropped when you set a multi-index, reset the index before this line:
# unodc = unodc.reset_index()
# unodc_avg["CODE"] = unodc["CODE"]
unodc_avg_year.loc[["United States","Sweden","Austria","Mexico","Morocco","Serbia"]]
column_name="crime_score"
data = [ dict(
        type = 'choropleth',
        locations = unodc_avg['CODE'],
        z = unodc_avg['crime_score'],
        text = unodc.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_avg[["crime_score"]].sort_values(by="crime_score",ascending=False)[:10]
import numpy as np
corruption=pd.read_csv("/content/theglobaleconomy_corruption.csv", sep=", ")
corruption["Rule of law"]=corruption["Rule of law"].astype(float)

corruption["Corruption perceptions - Transparency International"]=corruption["Corruption perceptions - Transparency International"].str.replace(",","").fillna(-1).astype(int).replace(-1, np.nan)
corruption.set_index(["Country","Code","Year"],inplace=True)
corruption_avg=pd.DataFrame()
corruption
corruption_avg["Rule of law"]=2.5-corruption["Rule of law"].unstack(level=2).mean(axis=1)
corruption_avg["Corruption Perception"]=100-corruption["Corruption perceptions - Transparency International"].unstack(level=2).mean(axis=1)
corruption_avg[["Rule of law","Corruption Perception"]]-=corruption_avg[["Rule of law","Corruption Perception"]].mean(axis=0)
corruption_avg[["Rule of law","Corruption Perception"]]/=corruption_avg[["Rule of law","Corruption Perception"]].std(axis=0)
corruption_avg["corruption_score"]=corruption_avg.mean(axis=1)
corruption_avg.reset_index(inplace=True)
corruption_avg.set_index(["Country"],inplace=True)
corruption_avg[:10]
column_name="corruption"
data = [ dict(
        type = 'choropleth',
        locations = corruption_avg['Code'],
        z = corruption_avg['corruption_score'],
        text = corruption_avg.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
corruption_avg[["corruption_score"]].sort_values(by="corruption_score",ascending=False)[:10]
import pandas as pd
import numpy as np

# Correct file path definition
file_path = '/content/theglobaleconomy_corruption.csv'
unodc_avg = pd.read_csv(file_path)

# Clean up column names by stripping any leading/trailing whitespace
unodc_avg.columns = unodc_avg.columns.str.strip()

# Convert columns to numeric, coercing errors to NaN
unodc_avg["Corruption perceptions - Transparency International"] = pd.to_numeric(unodc_avg["Corruption perceptions - Transparency International"], errors='coerce')
unodc_avg["Rule of law"] = pd.to_numeric(unodc_avg["Rule of law"], errors='coerce')

# Calculate average corruption and crime scores by 'Code'
corruption_avg = unodc_avg.groupby("Code")["Corruption perceptions - Transparency International"].mean().reset_index()
corruption_avg.rename(columns={'Corruption perceptions - Transparency International': 'corruption_score'}, inplace=True)

crime_avg = unodc_avg.groupby("Code")["Rule of law"].mean().reset_index()
crime_avg.rename(columns={'Rule of law': 'crime_score'}, inplace=True)

# Merge the average scores by 'Code'
unodc_w = corruption_avg.merge(crime_avg, on="Code").set_index("Code")

# Calculate the weighted crime score
unodc_w["crime_score_w"] = unodc_w["crime_score"] + (unodc_w["corruption_score"] / 3)

# Sort by 'crime_score_w' in descending order and display the top 10
top_10_countries = unodc_w.sort_values(by="crime_score_w", ascending=False).head(10)
print(top_10_countries)

column_name="crime_score_including_corruption"
data = [ dict(
        type = 'choropleth',
        # Use unodc_w.index to access the 'Code' values (which are now the index)
        locations = unodc_w.index,
        z = unodc_w['crime_score_w'],
        text = unodc_w.index,
        colorscale =[[0.0, 'rgb(255,255,255)'],[1e-6, 'rgb(237,235,242)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\
            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']],
        autocolorscale = False,
        reversescale = False,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            #autotick = False,
            title = column_name.replace("_"," ")),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map' )
#import plotly.io as pio
#pio.write_image(fig, 'world_avg_cr.pdf')
unodc_max_cat["max_cat"]=unodc_avg_year[fields].idxmax(axis=1)
unodc_max_cat["CODE"]=unodc["CODE"]
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat"].map(dict((f[1],f[0]+1)for f in enumerate(fields)))
unodc_max_cat["max_cat_ind"]=unodc_max_cat["max_cat_ind"].fillna(0)
column_name="world_max_crime_category"
colors=['#ffffff','#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe',
'#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#000000']
data = [ dict(
        type = 'choropleth',
        locations = unodc_max_cat['CODE'],
        z = unodc_max_cat['max_cat_ind'],
        text = unodc_max_cat["max_cat"].str.replace("_"," "),
        colorscale = [[i/11, colors[i]] for i in range(0,12)],
        autocolorscale = False,
        reversescale = False,
        show_scale=True,
        marker = dict(
            line = dict (
                color = 'rgb(180,180,180)',
                width = 0.5
            ) ),
        colorbar = dict(
            autotick = True,
            title = "colors"
        ),
      ) ]

layout = dict(
    title = 'world plot of '+column_name.replace("_"," "),
    geo = dict(
        showframe = False,
        showcoastlines = False,
        projection = dict(
            type = 'Mercator'
        )
    )
)

fig = dict( data=data, layout=layout )
py.iplot( fig, validate=False, filename='d3-world-map2' )
unodc_max_cat[["max_cat"]].loc[["Austria","United States","United Kingdom","Serbia","Croatia","India"]]
 df8 = pd.read_csv('/content/world_economic_indicators.csv (2).zip', encoding='latin1')
df8.head()
import pandas as pd

# Assuming df8 is your Pandas DataFrame
print(df8.info())
print(df8.describe())
df8_cleaned = df8.dropna(subset=['GDP (current US$)_x'])
import pandas as pd

# Assuming df8 is your Pandas DataFrame and you want to calculate the mean of 'GDP (current US$)_x'
mean_gdp = df8['GDP (current US$)_x'].mean()  # Accessing the column from the DataFrame
df8['GDP (current US$)_x'].fillna(mean_gdp, inplace=True)
df8['GDP (current US$)_x'].fillna(method='ffill', inplace=True)
df8['GDP (current US$)_x'].interpolate(method='linear', inplace=True)
print(df8.head())
country_code = 'USA'
data_country = df8[df8['Country Code'] == country_code]

plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP (current US$)_x'], marker='o', linestyle='-')
plt.title(f'GDP Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP (current US$)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['GDP growth (annual %)_x'], marker='o', linestyle='-', color='green')
plt.title(f'GDP Growth Rate for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('GDP Growth Rate (%)')
plt.grid(True)
plt.show()
plt.figure(figsize=(10, 6))
plt.plot(data_country['Year'], data_country['Unemployment, total (% of total labor force)'], marker='o', linestyle='-', color='red')
plt.title(f'Unemployment Trend for {country_code} (1960 - 2022)')
plt.xlabel('Year')
plt.ylabel('Unemployment (% of total labor force)')
plt.grid(True)
plt.show()
df_total_pop = pd.read_csv('/content/Total_population_both_sexes.csv',
                           sep=';', encoding='latin', decimal = ',',skiprows=[0])
df_total_pop.head()
df_total_pop_formatted = pd.melt(df_total_pop, ['Country'], var_name = 'Year', value_name = 'Total population in 1,000')
df_total_pop_formatted.columns = ['Region', 'Year', 'Total population in 1,000']
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
df_total_pop_formatted.Year = df_total_pop_formatted.Year.astype(int)
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].str.replace(' ', '')
df_total_pop_formatted.iloc[:,2] = df_total_pop_formatted.iloc[:,2].astype(np.float64)
df_total_pop_formatted = df_total_pop_formatted.loc[df_total_pop_formatted['Year'] >1999,:]
country_names2 = country_names.drop_duplicates()
false_names = ['Ivory Coast+A168', 'Viet Nam','Cabo Verde']
new_names = ['Ivory Coast', 'Vietnam','Cape Verde']
df_total_pop_formatted = df_total_pop_formatted.replace(to_replace=false_names, value=new_names)
df_total_pop_formatted = df_total_pop_formatted.sort_values(by = ['Region','Year'])
country_names = df_total_pop_formatted.Region
country_names2 = country_names.drop_duplicates()
df_pop_density = pd.read_csv('/content/Population_density.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_pop_density_formatted = pd.melt(df_pop_density, ['Country'], var_name = 'Year', value_name = 'Population density (persons per km2)')
df_pop_density_formatted.columns = ['Region', 'Year','Population density (persons per km2)']
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])
df_pop_density_formatted.Year = df_pop_density_formatted.Year.astype(int)
df_pop_density_formatted = df_pop_density_formatted.loc[df_pop_density_formatted['Year'] >1999,:]
country_names3 = df_pop_density_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao','Czech Republic', 'Ivory Coast', 'Korea', 'Falkland Islands','Holy See',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Sint Maarten', 'Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_pop_density_formatted = df_pop_density_formatted.replace(to_replace=false_names, value=new_names)
df_pop_density_formatted = df_pop_density_formatted.sort_values(by = ['Region','Year'])

country_names2.loc[country_names2.index.max() + 1] = 'Holy See'
df_crude_birth_rate = pd.read_csv('/content/Crude_birth_rate.csv',sep=';',encoding='latin',decimal = ',',skiprows = [0])
df_crude_birth_rate.head()
cols = df_crude_birth_rate.columns
col_loc = 0
for year in cols:
    if 'Country' in year:
        colloc=+1
        continue
    else:
        curr_col = df_crude_birth_rate[year]
        new_col = curr_col/5
        # Check if the year string contains a hyphen before splitting
        if '-' in year:
            first, last = year.split('-')
            for i in range(int(first),int(last)):
                df_crude_birth_rate[str(i)] = new_col
        else:
            # Handle cases where the year string doesn't have a hyphen
            # (e.g., assign the entire year string to first and last)
            first = year
            last = year
            # Or, you could print a warning or skip the column:
            # print(f"Warning: Skipping column '{year}' as it doesn't contain a hyphen.")
            # continue

        # Use axis=1 to specify column deletion
        df_crude_birth_rate = df_crude_birth_rate.drop(year, axis=1)

        col_loc=+1
df_crude_birth_rate[str(i+1)] = new_col
df_crude_birth_rate_formatted = pd.melt(df_crude_birth_rate, ['Country'], var_name = 'Year', value_name = 'Crude birth rate (births per 1,000)')
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Country','Year'])
df_crude_birth_rate_formatted.columns = ['Region', 'Year', 'Crude birth rate (births per 1,000)']
df_crude_birth_rate_formatted.Year = df_crude_birth_rate_formatted.Year.astype(int)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.loc[df_crude_birth_rate_formatted['Year'] >1999,:]
country_names3 = df_crude_birth_rate_formatted.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bolivia','Cape Verde','Hong Kong', 'Macao', 'Taiwan', 'Curacao', 'Czech Republic','Ivory Coast', 'Korea',
             'Iran', 'Laos', 'Micronesia','Moldova','Russia','Syria', 'Macedonia','Tanzania', 'Venezuela','Vietnam']
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.replace(to_replace=false_names, value=new_names)
df_crude_birth_rate_formatted = df_crude_birth_rate_formatted.sort_values(by = ['Region','Year'])
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep = ',') # Remove the delimiter argument. If comma is the intended delimiter, you only need 'sep'. If a different delimiter is intended, use 'delimiter' instead.
df_fertility.head()
# Load the file without specifying usecols to inspect the column names
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',')

# Display the column names
print(df_fertility.columns)

import pandas as pd

# Read the CSV file, inferring the header from the first row
df_fertility = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', header=0)

# Print the columns to check their names and indices
print(df_fertility.columns)

# Check the actual number of columns available
num_columns = len(df_fertility.columns)
print(f"Number of columns: {num_columns}")

# Instead of selecting specific columns, use all columns:
df_fertility = df_fertility[df_fertility.columns]

# Assuming your CSV has 'Country or Area', 'Year', and 'Value' as the first 3 columns
# Rename the columns to your desired names
df_fertility = df_fertility.rename(columns={'Country or Area': 'Country', 'Year': 'Year', 'Value': 'Total fertility rate'})
#df_fertility.columns = ['Country', 'Year', 'Total fertility rate']


# Print the columns again to verify the changes
print(df_fertility.columns)
import pandas as pd

# Initialize a list to store each DataFrame slice
df_list = [df_fertility.loc[17:29, :].copy()]

# Append each subsequent slice to the list
for k in range(1, 240):
    df_list.append(df_fertility.loc[k * 30 + 17 : k * 30 + 29, :])

# Concatenate all slices into a single DataFrame
df_fertility2 = pd.concat(df_list, ignore_index=True)

print(df_fertility2.columns)


print(df_fertility2.columns)
print(df_fertility2.head())

# Reload df_fertility2 with adjusted parsing options
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', sep=',', quotechar='"', engine='python')

# Verify the columns
print(df_fertility2.columns)

# Reload the CSV file with the correct delimiter and embedded quotes handling
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=",", quotechar='"')

# Clean up column names by removing extra whitespace and any quotes
df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)


df_fertility2.columns = df_fertility2.columns.str.strip()

# Display the corrected column names to confirm
print("Columns after cleanup:", df_fertility2.columns)

# Load the CSV without using the header
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None)

# Extract the first row as column headers and set them as columns
df_fertility2.columns = df_fertility2.iloc[0].str.replace('"', '').str.strip()
df_fertility2 = df_fertility2[1:]  # Remove the header row from the data

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# Display the cleaned DataFrame to verify the structure
print("Columns after manual header setup:", df_fertility2.columns)
df_fertility2.head()

import pandas as pd

# Load the file while specifying comma as the delimiter and handling quotation marks
df_fertility2 = pd.read_csv('/content/Total_fertility_rate (1).csv', delimiter=',', quotechar='"', engine='python')

# Check the columns to ensure they are parsed correctly
print(df_fertility2.columns)

import pandas as pd

# Load the CSV file without headers
df_fertility2 = pd.read_csv("/content/Total_fertility_rate (1).csv", header=None, delimiter=',', quotechar='"', engine='python')

# Extract the first row as headers and split it manually
# The issue was that the first row was being treated as a single column
# Instead of splitting the first element, we need to access the entire row
header_row = df_fertility2.iloc[0].str.replace('"', '').str.strip().tolist()  # Convert to a list

# Assign these as column headers
df_fertility2.columns = header_row

# Remove the original header row from the data
df_fertility2 = df_fertility2[1:]

# Reset the index for clean data
df_fertility2 = df_fertility2.reset_index(drop=True)

# (Optional) Rename columns if needed to match your specific requirements
# df_fertility2.columns = ['Country', 'Year', 'Variant', 'Total fertility rate']

# Display the cleaned DataFrame to verify structure
print("Columns after manual header setup:", df_fertility2.columns)
print(df_fertility2.head())
# Split the single column into separate columns based on commas
df_fertility2[['Country', 'Year', 'Variant', 'Total fertility rate']] = df_fertility2.iloc[:, 0].str.split(',', expand=True)

# Drop the original combined column
df_fertility2 = df_fertility2.drop(df_fertility2.columns[0], axis=1)

# Remove extra quotation marks from all string values
df_fertility2 = df_fertility2.applymap(lambda x: x.strip('"') if isinstance(x, str) else x)

# Create a new DataFrame to hold the expanded data
df_fertility3 = pd.DataFrame(columns=['Country', 'Year', 'Total fertility rate'])

# Iterate over each row in the cleaned DataFrame
for index, row in df_fertility2.iterrows():
    region = row['Country']
    year = row['Year']
    val = row['Total fertility rate']

    # Split the year range and expand it into individual years
    first, last = year.split('-')
    for i in range(int(first), int(last)):
        new_row = pd.DataFrame([[region, i, val]], columns=['Country', 'Year', 'Total fertility rate'])
        df_fertility3 = pd.concat([df_fertility3, new_row])

# Sort the DataFrame and adjust data types
df_fertility3 = df_fertility3.sort_values(by=['Country', 'Year'])
df_fertility3['Year'] = df_fertility3['Year'].astype(int)

# Filter years greater than 1999
df_fertility3 = df_fertility3.loc[df_fertility3['Year'] > 1999, :]

# Extract unique country/region names from df_fertility3
country_names3 = df_fertility3['Country'].drop_duplicates()

# Assuming 'country_names2' is a Series or list containing valid country names
# Identify names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2)]

# Convert the result to a list
false_names = false_names.tolist()

# Display the false names
false_names

# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")
# Removing quotes from the 'Year' column, if they exist and the column is of type string
# First, check if the column is of type object (which could hold strings)
if df_fertility3['Year'].dtype == object:
    # If it's an object, try to remove quotes
    df_fertility3['Year'] = df_fertility3['Year'].str.replace('"', '')
# If it's not an object (likely numeric), you don't need to remove quotes
# as numeric types don't store quotes.
# If you still want to handle potential errors, you could add a try-except block
else:
    try:
        df_fertility3['Year'] = df_fertility3['Year'].astype(str).str.replace('"', '').astype(int)
    except AttributeError:
        print("Warning: 'Year' column is not of string type, skipping quote removal.")
# First, split the combined string into separate components
# Assuming the column to split is the first one (index 0)
# Update the column index if it's a different column
split_columns = df_fertility3.iloc[:, 0].str.split(',', expand=True)

# Check the number of columns produced after the split
num_columns_after_split = split_columns.shape[1]

# Define the desired column names
desired_columns = ['Country or Area', 'Year', 'Variant', 'Value']

# If the number of columns after split matches the desired number of columns, rename them
if num_columns_after_split == len(desired_columns):
    split_columns.columns = desired_columns

    # Assign these split columns back to the original DataFrame
    df_fertility3[['Country or Area', 'Year', 'Variant', 'Value']] = split_columns

    # Drop the original combined column
    # Assuming the original column was the first one (index 0)
    df_fertility3 = df_fertility3.drop(columns=[df_fertility3.columns[0]])
else:
    # If the split doesn't produce the expected columns, handle the situation:
    print(f"Warning: The split operation resulted in {num_columns_after_split} columns, expected {len(desired_columns)}.")
    print("The column names were not assigned, and the original column was not dropped.")
    print("Please check the data in the column you are trying to split.")

    # Example of handling the issue:
    # 1. Inspect the 'split_columns' DataFrame to see how the data was split.
    # 2. Adjust the split logic or desired_columns accordingly.
    # 3. You might need to manually extract the desired information
    #    from the 'split_columns' DataFrame and assign it to the
    #    'Country or Area' column in 'df_fertility3'.

    # For demonstration purposes, let's assume the 'Country or Area'
    # information is in the first column of 'split_columns':
    df_fertility3['Country or Area'] = split_columns.iloc[:, 0]
# Define your lists of names to be replaced and new names
# Ensure both lists have the same length
false_names = ['Country1', 'Country2', 'Country3', 'Country4', 'Country5']  # Replace with actual old names
new_names = ['Bolivia', 'Cape Verde', 'Curacao', 'Czech Republic', 'Korea']  # Corresponding new names


# If you have more names in 'new_names', you might need to truncate it to match the length of 'false_names'
# new_names = new_names[:len(false_names)]

# Or, if you have more names in 'false_names', you might need to add corresponding values to 'new_names'
# This would require you to identify what should replace those extra names in 'false_names'

# Perform the replacement
df_fertility3['Country or Area'] = df_fertility3['Country or Area'].replace(to_replace=false_names, value=new_names)
# Sorting the DataFrame
df_fertility3 = df_fertility3.sort_values(by=['Country or Area', 'Year'])

# Assuming 'country_names2' is a DataFrame with a column 'Country'
country_names2 = pd.DataFrame(['Country1', 'Country2', 'Country3'], columns=['Country'])  # Example data

# Adding 'Polynesia' as a new row
country_names2.loc[country_names2.shape[0]] = 'Polynesia'

# Display the first few rows of the modified DataFrame
print(df_fertility3.head())
print(country_names2)

df_education_grade1 = pd.read_csv('/content/Enrolment_in_Grade_1_of_primary_education1.csv', sep = ",", usecols = (0,1,2,5))
df_education_grade1.columns = ['Region', 'Year', 'Sex', 'Enrolment in Grade 1 of primary education']
df_education_grade1 = df_education_grade1.loc[df_education_grade1['Year'] >1999,:]
#df_education_grade1
country_names3 = df_education_grade1.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
# Create a dictionary mapping the old country names to the new ones
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])
new_names = ['Ivory Coast','Korea','Hong Kong', 'Laos', 'Lybia','Macao','Netherlands Antilles','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary for replacement, ensuring it handles cases where there's no match
df_education_grade1['Region'] = df_education_grade1['Region'].map(name_mapping).fillna(df_education_grade1['Region'])

# Sort the DataFrame as before
df_education_grade1 = df_education_grade1.sort_values(by=['Region', 'Year'])

df_education_grade1 = df_education_grade1.sort_values(by = ['Region','Year'])
country_names2.loc[country_names2.shape[0]] = ['Netherlands Antilles']  # Append to the next available index
country_names2.loc[country_names2.shape[0]] = ['Palestine']
country_names2.loc[country_names2.shape[0]] = ['Réunion']
# The following lines were causing the error and have been replaced with .loc
#country_names2 = country_names2.set_value(country_names2.size,'Palestine')
#country_names2 = country_names2.set_value(country_names2.size,'Réunion')
# Use .loc to add the values, similar to the first three lines
country_names2.loc[country_names2.shape[0]] = ['Palestine'] # Add 'Palestine' to the next available index
country_names2.loc[country_names2.shape[0]] = ['Réunion']  # Add 'Réunion' to the next available index
df_pop_ed = pd.merge(df_total_pop_formatted,df_education_grade1, on=['Region','Year'], how='inner')
pop = df_pop_ed.iloc[:,2]
ed = df_pop_ed.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed['Enrolment in first grade of primary education (rate in percent)'] = ed_rate
df_education_grade1 = df_pop_ed.drop(['Total population in 1,000', 'Enrolment in Grade 1 of primary education'],axis=1)
df_education_grade1_female = df_education_grade1.loc[df_education_grade1['Sex'] == 'Female',:]
df_education_grade1_female = df_education_grade1_female.drop(['Sex'], axis=1)
df_education_grade1_female.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, females)']

df_education_grade1_all_genders = df_education_grade1.loc[df_education_grade1['Sex'] == 'All genders',:]
df_education_grade1_all_genders = df_education_grade1_all_genders.drop(['Sex'], axis=1)
df_education_grade1_all_genders.columns = ['Region', 'Year', 'Enrolment in first grade of primary education (rate in percent, all genders)']
df_education_drop_out = pd.read_csv('/content/Primary_education_Drop-out_rate1.csv', usecols = (0,1,2,5))
df_education_drop_out.columns = ['Region','Year', 'Sex', 'Primary Education drop-out rate (in percent)']
df_education_drop_out.Year = df_education_drop_out.Year.astype(int)
df_education_drop_out.iloc[:,3] = df_education_drop_out.iloc[:,3].astype(np.float64)
df_education_drop_out = df_education_drop_out.loc[df_education_drop_out['Year'] >1999,:]
country_names3 = df_education_drop_out.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Ivory Coast','Hong Kong','Laos','Macao','Palestine','Russia', 'Syria', 'Macedonia',
             'Tanzania', 'Venezuela','Vietnam']
# Create a dictionary mapping the false names to the new names.
name_mapping = dict(zip(false_names, new_names))

# Use the dictionary in the replace function.
df_education_drop_out['Region'] = df_education_drop_out['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_drop_out = df_education_drop_out.sort_values(by=['Region', 'Year'])
df_education_drop_out = df_education_drop_out.sort_values(by = ['Region','Year'])
df_education_drop_out_female = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Female',:]
df_education_drop_out_female = df_education_drop_out_female.drop(['Sex'], axis=1)
df_education_drop_out_female.columns = ['Region','Year','Primary Education drop-out rate (females)']

df_education_drop_out_male = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'Male',:]
df_education_drop_out_male = df_education_drop_out_male.drop(['Sex'], axis=1)
df_education_drop_out_male.columns = ['Region','Year','Primary Education drop-out rate (in percent, males)']

df_education_drop_out_all_genders = df_education_drop_out.loc[df_education_drop_out['Sex'] == 'All genders',:]
df_education_drop_out_all_genders = df_education_drop_out_all_genders.drop(['Sex'], axis=1)
df_education_drop_out_all_genders.columns = ['Region','Year','Primary Education drop-out rate (in percent, all genders)']
df_education_graduates = pd.read_csv('/content/Graduates_from_tertiary_education_both_sexes1.csv',usecols = (0,1,2,5))
df_education_graduates.columns = ['Region', 'Year', 'Sex','Graduates from tertiary education']
df_education_graduates = df_education_graduates.loc[df_education_graduates['Year'] >1999,:]
country_names3 = df_education_graduates.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Hong Kong', 'Laos','Macao','Palestine','Russia', 'Syria','Macedonia','United Kingdom','Tanzania', 'Venezuela','Vietnam']
name_mapping = {
    "CÃ´te d'Ivoire": 'Ivory Coast',
    "China, Hong Kong SAR": 'Hong Kong',
    "Lao PDR": 'Laos',
    "China, Macao SAR": 'Macao',
    "State of Palestine": 'Palestine',
    "Russian Federation": 'Russia',
    "Syrian Arab Republic": 'Syria',
    "North Macedonia": 'Macedonia',
    "United Republic of Tanzania": 'Tanzania',
    "Venezuela (Bolivarian Republic of)": 'Venezuela',
    "Viet Nam": 'Vietnam',
    # Add more mappings as needed
}

# Use the dictionary in the replace function.
df_education_graduates['Region'] = df_education_graduates['Region'].replace(name_mapping)

# Sort the DataFrame.
df_education_graduates = df_education_graduates.sort_values(by=['Region', 'Year'])
df_education_graduates = df_education_graduates.sort_values(by = ['Region','Year'])
df_pop_ed1 = pd.merge(df_total_pop_formatted,df_education_graduates, on=['Region','Year'], how='inner')
pop = df_pop_ed1.iloc[:,2]
ed = df_pop_ed1.iloc[:,4]
ed_rate = ed/(pop*1000)*100
df_pop_ed1['Graduates from tertiary education (rate in percent)'] = ed_rate
df_education_graduates = df_pop_ed1.drop(['Total population in 1,000', 'Graduates from tertiary education'],axis=1)
df_education_graduates_female = df_education_graduates.loc[df_education_graduates['Sex'] == 'Female',:]
df_education_graduates_female = df_education_graduates_female.drop(['Sex'], axis=1)
df_education_graduates_female.columns = ['Region','Year','Graduates from tertiary education (rate in percent, females)']

df_education_graduates_all_genders = df_education_graduates.loc[df_education_graduates['Sex'] == 'All genders',:]
df_education_graduates_all_genders = df_education_graduates_all_genders.drop(['Sex'], axis=1)
df_education_graduates_all_genders.columns = ['Region','Year','Graduates from tertiary education (rate in percent, all genders)']
df_edu = pd.read_csv('/content/education_theglobaleconomy.csv',usecols=(0,2,3),na_values=' ')
df_edu.rename(columns = {'Country':'Region',
                            ' Female to male ratio students at tertiary level education':
                             'Female to male ratio students at tertiary level education'},inplace=True)
df_edu = df_edu.loc[df_edu['Year'] <2016,:]
country_names3 = df_edu.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Brunai Darussalam','Myanmar','North Korea','Congo','Korea']
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
# In this way any name in false_names without a correspondent name in new_names will be mapped to None and therefore won't be changed.
# If you instead want to remove rows with names in false_names without a match in new_names you should substitute [None] * (len(false_names) - len(new_names)) with [] and proceed with the following code:
# df_edu = df_edu[df_edu['Region'].map(name_mapping).notna()]

# Now apply the mapping using the replace method
df_edu['Region'] = df_edu['Region'].replace(name_mapping)

# Sort the DataFrame
df_edu = df_edu.sort_values(by=['Region', 'Year'])
df_edu = df_edu.sort_values(by = ['Region','Year'])
names_dens = df_pop_density_formatted.Region.drop_duplicates()
names_dens = names_dens.tolist()

names_birth = df_crude_birth_rate_formatted.Region.drop_duplicates()
names_birth = names_birth.tolist()

names_fert = df_fertility3.Country.drop_duplicates() # Changed 'Region' to 'Country'
names_fert = names_fert.tolist()
names_ed1 = df_education_grade1_all_genders.Region.drop_duplicates()
names_ed1 = names_ed1.tolist()

names_ed_drop = df_education_drop_out_all_genders.Region.drop_duplicates()
names_ed_drop = names_ed_drop.tolist()

names_edu = df_edu.Region.drop_duplicates()
names_edu = names_edu.tolist()

names1 = [itm for itm in country_names2 if itm in names_dens]
names2 = [itm for itm in names_birth if itm in names_fert]
names3 = [itm for itm in names_ed1 if itm in names_ed_drop]
names4 = [itm for itm in names1 if itm in names2]
names5 = [itm for itm in names3 if itm in names4]
names = [itm for itm in names5 if itm in names_edu]

df_pop = df_total_pop_formatted[df_total_pop_formatted.Region.isin(names)==True]
df_dens = df_pop_density_formatted[df_pop_density_formatted.Region.isin(names)==True]
df_birth = df_crude_birth_rate_formatted[df_crude_birth_rate_formatted.Region.isin(names)==True]
df_fert = df_fertility3[df_fertility3.Country.isin(names)==True]
df_ed1 = df_education_grade1_all_genders[df_education_grade1_all_genders.Region.isin(names)==True]
df_drop_out = df_education_drop_out_all_genders[df_education_drop_out_all_genders.Region.isin(names)==True]
#df_grad = df_education_graduates_all_genders[df_education_graduates_all_genders.Region.isin(names)==True]
df_edu2 = df_edu[df_edu.Region.isin(names)==True]
df_social = pd.merge(df_pop, df_dens, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_birth, on = ['Region','Year'], how = 'outer')
# Changed 'Region' to 'Country' in the 'right_on' parameter for df_fert
df_social = pd.merge(df_social, df_fert, left_on=['Region','Year'], right_on=['Country','Year'], how = 'outer')
df_social = pd.merge(df_social, df_ed1, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_drop_out, on = ['Region','Year'], how = 'outer')
#df_social = pd.merge(df_social, df_grad, on = ['Region','Year'], how = 'outer')
df_social = pd.merge(df_social, df_edu2, on = ['Region','Year'], how = 'outer')
!pip install fancyimpute
import pandas as pd
# Import necessary modules
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp_cols = df_social.iloc[:,2:9].columns.values
to_impute = df_social.iloc[:,2:9]

# Check if 'to_impute' is empty
if to_impute.empty:
    print("Warning: DataFrame 'to_impute' is empty. Skipping imputation.")
else:
    # Now IterativeImputer should be recognized
    df_social.iloc[:,2:9] = pd.DataFrame(IterativeImputer().fit_transform(to_impute),columns = imp_cols)

df_social = df_social.sort_values(by = ['Region','Year'])
df_social = df_social.set_index(['Region','Year'])
df_social.isna().sum().sum()
# Assuming 'df_social' contains the region data you need
country_names3 = df_social.reset_index().Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
df8 = df_fertility3  # Assuming df_fertility3 is the DataFrame you want to use

# Resetting the index and extracting the 'Country' column (Assuming 'Country' represents the region in df8)
country_names3 = df8.reset_index()['Country']

# Dropping duplicate region names
country_names3 = country_names3.drop_duplicates()

# Identifying names in country_names3 that are not in country_names2
false_names = country_names3[~country_names3.isin(country_names2['Country'])]  # Ensure 'Country' is the column name in country_names2

# Converting to a list
false_names = false_names.tolist()
false_names
# Instead of directly resetting the index, drop the 'level_0' column if it exists before resetting the index.
if 'level_0' in df8.columns:
    df8 = df8.drop(columns=['level_0'])
#Now resetting the index
df8 = df8.reset_index()

# Creating a dictionary to map old names to new names
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))
#The above line maps false_names with new_names and fills rest with None if new_names is shorter than false_names.

# Replacing the incorrect names with the correct names using the mapping dictionary
df8['Country'] = df8['Country'].replace(name_mapping)

df8 = df8.sort_values(by = ['Country','Year'])
econom_isi = df8[df8['Country'].isin(names)==True]
df8 = df8.set_index(['Country','Year'])
df8.head()
unodc_mult = unodc_mult.reset_index()
country_names3 = unodc_mult.Region
country_names3 = country_names3.drop_duplicates()
false_names = country_names3[country_names3.isin(country_names2)==False]
false_names = false_names.tolist()
false_names
new_names = ['Bahamas','Brunai Darussalam','Myanmar', 'Cape Verde','Congo democratic','Congo', 'Ivory Coast','Falkland Islands',
             'Faeroe Islands','Gambia','Guernsey','Hong Kong','Iran','Iraq','Jersey','North Korea','Korea','Kosovo',
             'Kosovo under UNSCR 1244','Macao','Macau','Micronesia','Sint Maarten',
            'Saint Kitts and Nevis','Saint Vincent and the Grenadines','Syria','Macedonia',
            'United Kingdom (Northern Ireland)','United Kingdom (Scotland)',
            'United States of America','British Virgin Islands','West Bank']
#unodc_mult = unodc_mult.reset_index()
if 'level_0' in unodc_mult.columns:
    unodc_mult = unodc_mult.drop(columns=['level_0'])

# Create a dictionary mapping false_names to new_names, handling unequal lengths
name_mapping = dict(zip(false_names, new_names + [None] * (len(false_names) - len(new_names))))

# Replace incorrect names with correct names using the mapping dictionary
unodc_mult['Region'] = unodc_mult['Region'].replace(name_mapping)

unodc_mult = unodc_mult.sort_values(by=['Region', 'Year'])
unodc_mult_isi = unodc_mult[unodc_mult['Region'].isin(names)]

unodc_mult = unodc_mult.set_index(['Region', 'Year'])
# Resetting index for all DataFrames
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in df_social.columns:
    df_social = df_social.drop(columns=['level_0'])
df_social = df_social.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in unodc_mult_isi.columns:
    unodc_mult_isi = unodc_mult_isi.drop(columns=['level_0'])
unodc_mult_isi = unodc_mult_isi.reset_index()
#Instead of directly resetting, drop 'level_0' if it exists, then reset
if 'level_0' in econom_isi.columns:
    econom_isi = econom_isi.drop(columns=['level_0'])
econom_isi = econom_isi.reset_index()


# Extracting unique 'Region' / 'Country' names and converting to lists
names_social = df_social['Region'].drop_duplicates().tolist()
names_uno = unodc_mult_isi['Region'].drop_duplicates().tolist()
names_econom = econom_isi['Country'].drop_duplicates().tolist()  # Using 'Country' for econom_isi

# Finding common names across all three lists
names = list(set(names_social) & set(names_econom) & set(names_uno))  # Efficient way to find common elements


# Filtering DataFrames based on common names
social_isi = df_social[df_social['Region'].isin(names)]
unodc_mult_isi = unodc_mult_isi[unodc_mult_isi['Region'].isin(names)]
econom_isi = econom_isi[econom_isi['Country'].isin(names)]  # Using 'Country' for econom_isi

# Dropping the 'index' column only if it exists in econom_isi
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore')


# Renaming 'Country' to 'Region' in econom_isi to ensure consistent column names for merging
econom_isi = econom_isi.rename(columns={'Country': 'Region'})

# Merging the DataFrames on 'Region' and 'Year'
df_all = pd.merge(unodc_mult_isi, econom_isi, on=['Region', 'Year'], how='outer')
df_all = pd.merge(df_all, social_isi, on=['Region', 'Year'], how='outer')

# Setting index for individual DataFrames
# Dropping 'index' column if it exists before setting index to avoid potential issues
df_social = df_social.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
unodc_mult_isi = unodc_mult_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])
econom_isi = econom_isi.drop(['index'], axis=1, errors='ignore').set_index(['Region', 'Year'])

# Dropping the 'index' column from df_all only if it exists
df_all = df_all.drop(['index'], axis=1, errors='ignore')

# Sorting the DataFrame by 'Region' and 'Year'
df_all = df_all.sort_values(by=['Region', 'Year'])

# Display the first few rows of the final merged DataFrame
df_all.head()
df_all = df_all.drop(['Assault_Abs', 'Kidnapping_Abs', 'Theft_Abs', 'Robbery_Abs',
       'Burglary_Abs', 'Domestic_Burglary_Abs', 'Theft_of_Private_Cars_Abs',
       'Motor_Vehicle_Theft_Abs', 'Total_Sexual_Violence_Abs', 'Rape_Abs',
       'Sexual_Offences_ag_Children_Abs'], axis = 1)
df_all.columns

df_all.rename(columns = {'Assault_Rel':'Assault rate per 100,000 population',
                        'Kidnapping_Rel':'Kidnapping rate per 100,000 population',
                        'Theft_Rel':'Theft rate per 100,000 population',
                        'Robbery_Rel':'Robbery rate per 100,000 population',
                        'Burglary_Rel':'Burglary rate per 100,000 population',
                        'Domestic_Burglary_Rel':'Domestic Burglary rate per 100,000 population',
                        'Theft_of_Private_Cars_Rel':'Theft of Private Cars rate per 100,000 population',
                        'Motor_Vehicle_Theft_Rel':'Motor Vehicle Theft rate per 100,000 population',
                        'Total_Sexual_Violence_Rel':'Total Sexual Violence rate per 100,000 population',
                        'Rape_Rel':'Rape rate per 100,000 population',
                        'Sexual_Offences_ag_Children_Rel':'Sexual Offences agains Children rate per 100,000 population'},
                inplace = True)
df_all_red = df_all.loc[df_all['Year'] > 2002,:]
df_all_red = df_all_red.loc[df_all_red['Year'] < 2014,:]
print('Those are the countries that are not suitable for interpolation')
c = list()
for country in names:
    curr_country = df_all_red.loc[df_all_red['Region']==country,:]
    first = curr_country.iloc[2,:]
    last = curr_country.iloc[10,:]

    if first.isna().sum() > 0:
        print(country)
        c.append(country)
        continue
    if last.isna().sum()  > 0:
        print(country)
        c.append(country)
len(c)
df_social_max=df_social
my_countries=unodc_mult.index.levels[0]
plot_countries=["United States","Sweden","Austria","Mexico","Croatia"]
fields_rel=[f+"_Rel" for f in fields]
unodc_avg_region_all=pd.DataFrame()
unodc_avg_region=unodc_mult.loc[my_countries][fields_rel]
unodc_avg_region-=unodc_avg_region.mean()
unodc_avg_region/=unodc_avg_region.std()
unodc_avg_region_all["crime_score_raw"]=unodc_avg_region.mean(axis=1)

unodc_avg_region_all["corruption_score"]=0
for cntr in unodc_avg_region_all.index.levels[0]:
    try:
        unodc_avg_region_all["corruption_score"].loc[cntr]=corruption_avg["corruption_score"].loc[cntr]
    except Exception:
        continue
unodc_avg_region_all["crime_score"]=unodc_avg_region_all["crime_score_raw"]+unodc_avg_region_all["corruption_score"]/4
#unodc_avg_region_all["crime_score"]-=unodc_avg_region_all["crime_score"].min()
unodc_avg_region_all[:20]
# Check if the columns actually exist in the DataFrame
print(unodc_avg_region_all.columns)

# Attempt to drop the columns, handling potential KeyError if they don't exist
try:
    unodc_avg_region_all.drop(columns=["corruption_score", "crime_score_raw"], inplace=True)
except KeyError:
    print("One or both columns not found in DataFrame. Skipping drop.")
import matplotlib.pyplot as plt

# Filter the DataFrame based on the 'Region' level of the MultiIndex
plot_data = unodc_avg_region_all.loc[
    unodc_avg_region_all.index.get_level_values('Region').isin(plot_countries)
]

# Check if the filtered DataFrame is empty
if plot_data.empty:
    print("No data found for the specified regions in plot_countries. Please check your filtering criteria.")
else:
    # Reset the index to flatten the MultiIndex
    plot_data = plot_data.reset_index()

    # Use the DataFrame's index for pivoting and plot
    pivot_data = plot_data.pivot(index=plot_data.index, columns='Region', values='crime_score')

    # Plot the data
    pivot_data.plot(figsize=(16, 9))
    plt.show()

print(unodc_avg_region_all.index.get_level_values('Region').unique())

# Normalize the formatting
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Apply the filtering with normalized values
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

import matplotlib.pyplot as plt

# Step 1: Print all available region names to understand their format
available_regions = unodc_avg_region_all.index.get_level_values('Region').unique()
print("Available regions in the data:", available_regions)

# Step 2: Normalize the region names and the plot_countries list
normalized_regions = available_regions.str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Step 3: Check if there are any common regions after normalization
matching_regions = [region for region in normalized_plot_countries if region in normalized_regions]
print("Matching regions:", matching_regions)

# Step 4: If there are matching regions, proceed with filtering; otherwise, report an error
if not matching_regions:
    print("No matching regions found. Please ensure plot_countries contains correct region names.")
else:
    # Filter the data using the matching regions
    filtered_data = unodc_avg_region_all.loc[
        unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip().isin(matching_regions)
    ]

    # Check if the filtered DataFrame is empty
    if filtered_data.empty:
        print("No data found for the specified regions in plot_countries after filtering. Check your data and criteria.")
    else:
        # Unstack the data to prepare for plotting
        unstacked_data = filtered_data['crime_score'].unstack(level=0)

        # Ensure there is numeric data to plot
        if unstacked_data.select_dtypes(include='number').empty:
            print("No numeric data to plot after unstacking. Check data types.")
        else:
            unstacked_data.plot(figsize=(16, 9))
            plt.show()

import matplotlib.pyplot as plt

# Manually update plot_countries with matching region names from the available list
# Example: Adjust the names to match your needs
plot_countries = [
    'Bahamas', 'British Virgin Islands', 'Cape Verde', 'Congo',
    'Falkland Islands', 'Iran', 'Korea', 'Macedonia', 'Syria'
]

# Normalize the region names in the DataFrame and the updated plot_countries list
normalized_regions = unodc_avg_region_all.index.get_level_values('Region').str.lower().str.strip()
normalized_plot_countries = [country.lower().strip() for country in plot_countries]

# Filter the data using normalized region names
filtered_data = unodc_avg_region_all.loc[
    normalized_regions.isin(normalized_plot_countries)
]

# Check if the filtered DataFrame is still empty
if filtered_data.empty:
    print("No data found for the specified regions after updating plot_countries. Please check your criteria.")
else:
    # Unstack the data to prepare for plotting
    unstacked_data = filtered_data['crime_score'].unstack(level=0)

    # Ensure there is numeric data to plot
    if unstacked_data.select_dtypes(include='number').empty:
        print("No numeric data to plot after unstacking. Check data types.")
    else:
        # Plot the data
        unstacked_data.plot(figsize=(16, 9))
        plt.title("Crime Score by Region")
        plt.xlabel("Index")
        plt.ylabel("Crime Score")
        plt.legend(title="Region")
        plt.show()

import numpy as np
models={}
unodc_avg_region_all["crime_score_lin"]=0
crime_score_grad=pd.Series()
for c in unodc_avg_region_all.index.levels[0]:
    tmp=unodc_avg_region_all.loc[c]
    tmp=tmp[~tmp["crime_score"].isna()]
    if tmp.shape[0]<12:
        continue
    alpha=np.polyfit(
        (tmp.index-2000).tolist(),
        tmp["crime_score"].tolist(),
        2
    )
    for y in range(3,15):
        unodc_avg_region_all["crime_score_lin"].loc[c,2000+y]=y*y*alpha[0]+y*alpha[1]+alpha[2]
    crime_score_grad[c]=abs(alpha[0]) # taking the coefficient of x**2 as this is telling wether or not there was a change
import matplotlib.pyplot as plt
unodc_avg_region_all.loc[plot_countries].unstack(level=0).plot(figsize=(16, 9))
interesting_countries=crime_score_grad.sort_values(ascending=False).head(7).index

import matplotlib.pyplot as plt
unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0).plot(figsize=(16, 9))
import matplotlib.pyplot as plt

# Assuming you want to analyze the trend of "crime_score" for interesting countries
trend_plot = unodc_avg_region_all["crime_score"].loc[interesting_countries].unstack(level=0)

# Now you can print the data types of the trend_plot DataFrame
print(trend_plot.dtypes)
# Standardize only numeric columns
numeric_cols = trend_plot.select_dtypes(include=['float64', 'int64']).columns
trend_plot[numeric_cols] -= trend_plot[numeric_cols].mean()
trend_plot[numeric_cols] /= trend_plot[numeric_cols].std()

# Fill missing values with 0, or use another appropriate fill value
trend_plot = trend_plot.fillna(0)

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code
df_econ_max = df_econ_max.reindex(trend_plot.index, fill_value=0)  # Reindex and fill missing values with 0

# Merging the dataframes
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer")

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
try:
    print(trend_plot.loc["Australia"])
except KeyError:
    print("Australia is not in the index.")

import pandas as pd

# Sample input data (replace with actual data loading code)
trend_plot = unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot = trend_plot.merge(df_social_max, left_index=True, right_index=True, how="outer")

# Assuming df_econ_max is loaded and indexed by 'Country'
df_econ_max = pd.DataFrame({'GDP': [100, 200, 300]}, index=['Country1', 'Country2', 'Country3'])  # Replace with actual loading code

# Assuming 'trend_plot' has a MultiIndex with levels 'Country' and 'Region'
# Replace 'SomeRegion' with the appropriate region for Australia
australia_index = pd.MultiIndex.from_tuples([('Australia', 'SomeRegion')], names=trend_plot.index.names)

# Reindex to include 'Australia'
df_econ_max = df_econ_max.reindex(trend_plot.index.union(australia_index), fill_value=0)

# Merging the dataframes and filling missing values
trend_plot = trend_plot.merge(df_econ_max, left_index=True, right_index=True, how="outer").fillna(0)

# Normalize trend_plot by subtracting mean and dividing by std (with adjustment to avoid division by zero)
trend_plot -= trend_plot.mean()
trend_plot /= trend_plot.std().replace(0, 1e-8)  # Add small constant to avoid division by zero

# Accessing data for 'Australia' (if it exists in the index)
if ("Australia", "SomeRegion") in trend_plot.index: # Access using the full MultiIndex tuple
    print(trend_plot.loc[("Australia", "SomeRegion")])
else:
    print("Australia is not in the index even after reindexing.")
trend_plot=unodc_avg_region_all[["crime_score"]].loc[interesting_countries].copy()
trend_plot=trend_plot.merge(df_social_max,left_index=True,right_index=True,how="outer")
trend_plot=trend_plot.merge(df_econ_max,left_index=True,right_index=True,how="outer")
trend_plot-=trend_plot.mean()
# Replace 0 standard deviations with 1 to avoid division by zero
trend_plot /= trend_plot.std().replace(0, 1)
trend_plot.loc["Australia"]
# Check which columns are present in trend_plot
print("Columns in trend_plot:", trend_plot.columns)

# List of columns you want to plot
columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

# Iterate through the interesting countries and plot only existing columns
for c in interesting_countries:
    # Select columns that exist in trend_plot
    data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Check if there's data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=c, figsize=(16, 9))
    else:
        print(f"No data available for {c}")

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    if c in trend_plot.index:
        data_to_plot = trend_plot.loc[c, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=c, figsize=(16, 9))
        else:
            print(f"No data available for {c}")
    else:
        print(f"{c} is not in the trend_plot index.")

# Optional: Check for mismatches in country names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Select columns that exist in trend_plot for each country
    standardized_name = country_name_mapping.get(c, c)  # Use mapped name if exists
    if standardized_name in trend_plot.index:
        data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

        # Plot only if data is available
        if not data_to_plot.empty:
            data_to_plot.plot(title=standardized_name, figsize=(16, 9))
        else:
            print(f"No data available for {standardized_name}")
    else:
        print(f"{standardized_name} is not in the trend_plot index.")

# Optional: Confirm all countries have matched names
missing_countries = set(interesting_countries) - set(trend_plot.index)
if missing_countries:
    print("These countries are not in trend_plot after renaming:", missing_countries)

# Define a mapping dictionary for country name standardization
country_name_mapping = {
    'Macau': 'Macau',
    'Macao': 'Macau',
    'Myanmar': 'Myanmar'
}

# Standardize country names in the trend_plot index
trend_plot = trend_plot.rename(index=country_name_mapping)

# List of columns to plot, adjusted based on available columns in trend_plot
columns_to_plot = [
    "crime_score",
    "GDP",
    "Total population in 1,000",
    "Population density (persons per km2)",
    "Primary Education drop-out rate (in percent, all genders)"
]

# Ensure all countries in interesting_countries are present in trend_plot
# Add any missing countries with NaN values
missing_countries = [country_name_mapping.get(c, c) for c in interesting_countries if country_name_mapping.get(c, c) not in trend_plot.index]
for country in missing_countries:
    trend_plot.loc[country] = [None] * len(trend_plot.columns)  # Fill with NaNs

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting if it exists in the standardized DataFrame
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot only if data is available; otherwise, notify
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No data available for {standardized_name}. Added empty entry for consistency.")

# Skip countries with all NaN or zero values
for c in interesting_countries:
    standardized_name = country_name_mapping.get(c, c)

    # Select data columns that exist and drop NaN values
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Filter out zero values (if any)
    data_to_plot = data_to_plot[data_to_plot != 0]

    # Check if there’s any remaining data to plot
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"Skipping {standardized_name} due to lack of meaningful data.")

# Define placeholder data for missing countries (example values, modify as needed)
placeholder_data = {
    "crime_score": 0,
    "GDP": 0,
    "Total population in 1,000": 0,
    "Population density (persons per km2)": 0,
    "Primary Education drop-out rate (in percent, all genders)": 0
}

# Assign placeholder data to missing countries for testing
for country in missing_countries:
    trend_plot.loc[country] = placeholder_data

# Iterate through interesting countries and plot available data
for c in interesting_countries:
    # Get standardized country name
    standardized_name = country_name_mapping.get(c, c)

    # Select data for plotting
    data_to_plot = trend_plot.loc[standardized_name, trend_plot.columns.intersection(columns_to_plot)].dropna()

    # Plot if data is available; notify if it's placeholder data
    if not data_to_plot.empty:
        data_to_plot.plot(title=standardized_name, figsize=(16, 9))
    else:
        print(f"No real data available for {standardized_name}. Plotting placeholder data.")

for c in interesting_countries:
    # Define the columns you want to plot
    columns_to_plot = ["crime_score", "GDP per capita", "Inflation rate", "Unemployment rate per capita", "Primary Education drop-out rate (in percent, all genders)"]

    # Get the columns that are actually available in trend_plot
    available_columns = [col for col in columns_to_plot if col in trend_plot.columns]

    # Standardize the country name using your mapping
    standardized_name = country_name_mapping.get(c, c)

    # Plot only the available columns for the standardized country name
    trend_plot.loc[standardized_name, available_columns].plot(title=standardized_name, figsize=(16, 9))
import pandas as pd
df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv")

print(df.head())

df = pd.read_csv("/content/Enrolment_in_Grade_1_of_primary_education1.csv", delimiter=";")

print(df.info())

crime_normalised=unodc_avg_region_all["crime_score"].reset_index()
df_all_red["total_crime"]=crime_normalised["crime_score"]
df_all_red.interpolate(method='linear',inplace=True)
df_all_red.fillna(method="bfill",inplace=True)
df_all_red.fillna(method="ffill",inplace=True)

df_all_red.head()
df_all_red.drop(['Region'],axis=1,inplace=True)

# calculate the correlation matrix
corr = df_all_red.corr()

# plot the heatmap
sns_plot2=sns.heatmap(corr,
        xticklabels=corr.columns,
        yticklabels=corr.columns)

df_all.head()
y_pred=df_all_red["total_crime"]
df_all_red.drop(['total_crime'], axis=1, inplace=True)
X_train=df_all_red.loc[df_all_red['Year'] < 2013,:]
X_test1=df_all_red.loc[df_all_red['Year'] > 2012,:]
X_test1.head()
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
n_fold = 5
folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)
def train_model(X=X_train, X_test=X_test1, y=y_pred, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        if model_type == 'lgb':
            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)
            model.fit(X_train, y_train,
                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',verbose=10000, early_stopping_rounds=200)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

        if model_type == 'xgb':
            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_tr.columns)
            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_tr.columns)

            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]
            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)
            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)
            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_tr.columns), ntree_limit=model.best_ntree_limit)

        if model_type == 'rcv':
            model = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0, 100.0, 1000.0), scoring='neg_mean_absolute_error', cv=5)
            model.fit(X_train, y_train)
            print(model.alpha_)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'sklearn':
            model = model
            model.fit(X_train, y_train)

            y_pred_valid = model.predict(X_valid).reshape(-1,)
            score = mean_absolute_error(y_valid, y_pred_valid)
            print(f'Fold {fold_n}. MAE: {score:.4f}.')
            print('')

            y_pred = model.predict(X_test).reshape(-1,)

        if model_type == 'cat':
            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params)
            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)

            y_pred_valid = model.predict(X_valid)
            y_pred = model.predict(X_test)

        oof[valid_index] = y_pred_valid.reshape(-1,)
        scores.append(mean_absolute_error(y_valid, y_pred_valid))

        prediction += y_pred

        if model_type == 'lgb':
            # feature importance
            fold_importance = pd.DataFrame()
            fold_importance["feature"] = X.columns
            fold_importance["importance"] = model.feature_importances_
            fold_importance["fold"] = fold_n + 1
            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

    prediction /= n_fold

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

    if model_type == 'lgb':
        feature_importance["importance"] /= n_fold
        if plot_feature_importance:
            cols = feature_importance[["feature", "importance"]].groupby("feature").mean().sort_values(
                by="importance", ascending=False)[:50].index

            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]

            plt.figure(figsize=(16, 12));
            sns.barplot(x="importance", y="feature", data=best_features.sort_values(by="importance", ascending=False));
            plt.title('LGB Features (avg over folds)');

            return oof, prediction, feature_importance
        return oof, prediction

    else:
        return oof, prediction

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
import time
import pandas as pd
import numpy as np

# ... (Your existing code for data loading and preprocessing) ...

def train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):
    """
    Trains a model using KFold cross-validation.

    Args:
        X: Training data (pandas DataFrame).
        X_test: Test data (pandas DataFrame).
        y: Target variable (pandas Series).
        params: Model parameters (dict).
        folds: KFold object.
        model_type: Type of model ('lgb', 'xgb', 'rcv', 'sklearn', 'cat').
        plot_feature_importance: Whether to plot feature importance (bool).
        model: Pre-trained model (optional).

    Returns:
        oof: Out-of-fold predictions (numpy array).
        prediction: Predictions on test data (numpy array).
        feature_importance: Feature importance (pandas DataFrame, if applicable).
    """

    # Check if X is empty and raise a more informative error
    if X.empty:
        raise ValueError("Input data 'X' is empty. Please check your data loading or preprocessing steps.")

    oof = np.zeros(len(X))
    prediction = np.zeros(len(X_test))
    scores = []
    feature_importance = pd.DataFrame()

    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):
        print('Fold', fold_n, 'started at', time.ctime())
        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        # ... (Rest of your train_model function code) ...

    return oof, prediction, feature_importance  # Return feature_importance if applicable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import r2_score, mean_squared_error
from sklearn import ensemble, tree, linear_model
from sklearn.model_selection import train_test_split, cross_val_score
import plotly.figure_factory as ff
import plotly.offline as py
import plotly.graph_objs as go
from plotly import tools

# Configurations
%matplotlib inline
sns.set_style('darkgrid')
warnings.filterwarnings("ignore")
py.init_notebook_mode(connected=True)

# Load dataset and handle errors if file or columns are missing
try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
except FileNotFoundError:
    print("Error: report.csv not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: File could not be parsed. Check if the CSV file is in the correct format.")
else:
    required_columns = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population", "agency_jurisdiction"]
    missing_columns = [col for col in required_columns if col not in df2.columns]
    if missing_columns:
        print(f"Error: Missing columns in dataset - {missing_columns}")
    else:
        # Adding total crimes column
        df2['total_crimes'] = df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]

        # Calculating relative crime rate
        df2["relative_crime"] = (df2["violent_crimes"] + df2["homicides"] + df2["rapes"] + df2["assaults"] + df2["robberies"]) / df2["population"]

# Visualization for total crimes
if 'total_crimes' in df2.columns and 'agency_jurisdiction' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='total_crimes', y='agency_jurisdiction', data=df2, estimator=sum, color="y").set(
        title='Total Number of Crimes in the US over 40 Years',
        xlabel='Total Number of Crimes', ylabel='City and State'
    )
    plt.show()

# Visualization for relative crime rate
if 'relative_crime' in df2.columns:
    plt.figure(figsize=(10, 22))
    sns.barplot(x='relative_crime', y='agency_jurisdiction', data=df2, color="b").set(
        title='Relative Crime Rate by Jurisdiction',
        xlabel='Relative Crime Rate', ylabel='City and State'
    )
    plt.show()

df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]] = df2[["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]].apply(pd.to_numeric, errors='coerce')

try:
    df2 = pd.read_csv("report.csv", encoding='latin1')
    # Convert columns to numeric
    cols_to_numeric = ["violent_crimes", "homicides", "rapes", "assaults", "robberies", "population"]
    df2[cols_to_numeric] = df2[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
except FileNotFoundError:
    print("Error: report.csv not found.")
except pd.errors.ParserError:
    print("CSV format error.")
